import time
import random
from datetime import datetime
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer
from confluent_kafka.avro import CachedSchemaRegistryClient
import string

# Configuration
TOPIC_NAME = "my-topic"
BOOTSTRAP_SERVERS = "35.192.128.128:9094"
SCHEMA_REGISTRY_URL = "http://104.198.190.159:8081"

# Define Avro schema
user_schema_str = """
{
    "namespace": "com.example",
    "type": "record",
    "name": "User",
    "fields": [
        {"name": "name", "type": "string"},
        {"name": "age", "type": "int"},
        {"name": "email", "type": "string"},
        {"name": "city", "type": "string"},
        {"name": "events_timestamp", "type": "long"}
    ]
}
"""

# Create Schema Registry Client
schema_registry_client = CachedSchemaRegistryClient({"url": SCHEMA_REGISTRY_URL})
value_schema = avro.loads(user_schema_str)

# Create producer config without 'key.serializer' or 'value.serializer'
producer_config = {
    'bootstrap.servers': BOOTSTRAP_SERVERS,
    'acks': 'all',  # Ack configuration (to ensure message delivery)
}

# Create the AvroProducer with the Schema Registry Client
producer = AvroProducer(producer_config, schema_registry=schema_registry_client)


# Helper functions
def random_city_picker():
    cities = [
        "Mumbai", "Delhi", "Bangalore", "Hyderabad", "Ahmedabad",
        "Chennai", "Kolkata", "Surat", "Pune", "Jaipur"
    ]
    return random.choice(cities)


def generate_avro_record():
    name = f"user-{random.randint(0, 999)}"
    age = random.randint(18, 88)
    email = f"{name}@example.com"
    events_time = int(datetime.now().timestamp() * 1000)
    address = ''.join(random.choices(string.ascii_uppercase + string.digits, k=25))

    return {
        "name": name,
        "age": age,
        "email": email,
        "city": random_city_picker(),
        "events_timestamp": events_time
    }


# Send messages in an infinite loop
try:
    while True:
        record_value = generate_avro_record()
        producer.produce(
            topic=TOPIC_NAME,
            key=None,
            value=record_value,
            value_schema=value_schema,
            callback=lambda err, msg: (
                print(f"Message sent to {msg.topic()} [partition {msg.partition()}] at offset {msg.offset()}")
                if err is None else print(f"Failed to send message: {err}")
            )
        )
        print(f"Produced record: {record_value}")
        time.sleep(1)  # Sleep for 1 second between records
except KeyboardInterrupt:
    print("Stopping producer...")
finally:
    producer.flush()
